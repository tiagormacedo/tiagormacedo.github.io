#     ARTICLES TO READ


Tao, _Machine assisted proofs_: http://www.ams.org/journals/notices/202501/rnoti-p6.pdf

    On computer-assisted mathematics: the instances in which machine learning, 
    proof assistants and large language models have advanced mathematics, and 
    the possibilities of combining these techniques in the future.

_Machine learning, mathematical theory and scientific applications_: http://www.ams.org/journals/notices/201911/rnoti-p1813.pdf

    The article begins with a summary of a few areas in which machine learning has
    been applied in science: partial differential equations, multi-scale modeling,
    molecular dynamics, gas dynamics, and natural language processing. Then, the
    mathematical theory behind regression problems is shortly presented. Two 
    important points that the article highlights:
    
      * "... in theoretical and computational science and engineering, a 
        fundamental obstacle that we have encountered is our limited ability
        to deal with problems in high dimension."
         
      * "To build the theoretical foundation of machine learning, we need to 
        develop high-dimensional numerical analysis."

Hernandez, _Symmetries of Grothendieck rings in representation theory_: http://arxiv.org/pdf/2501.03024

    The article starts with a broad overview on representation theory, 
    category theory, Grothendieck rings and cluster algebras in order to
    present several instances in which Grothendieck rings admit symmetries.

_Learning the symmetric group: large from small_: http://arxiv.org/pdf/2502.12717
    
    In this article, the authors show that a transformer neural-network trained 
    on predicting permutations in the symmetric group on 10 letters from their 
    words (either formed by general transpositions or by adjacent transpositions)
    can generalize to bigger symmetric groups with nearly 100% accuracy. They
    employ identity augmentation as a key tool to manage variable word lengths,
    and partitioned windows for training on the adjacent transpositions case.

---

_Machine Learning meets Algebraic Combinatorics_: http://arxiv.org/pdf/2503.06366

Jankowski, _Reduced superschemes and the combinatorics of toric supervarieties_: http://arxiv.org/pdf/2502.15977

_Tropical geometry and machine learning_: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9394420

    Conclusion. Tropical geometry and max-plus algebra offer a rich collection 
    of ideas and tools to model and solve problems in machine learning. In this
    work, we have surveyed the state of the art and some recent progress in 
    three areas: 1) deep neural networks with piecewise-linear activation 
    functions; 2) probabilistic graphical models and algorithms for weighted 
    finite-state transducers; and 3) nonlinear regression with piecewise-linear
    functions. Furthermore, we have introduced extensions to general max 
    algebras that allowed us to: 1) express the optimal solutions of several of
    the above problems as projections onto nonlinear vector spaces called 
    weighted lattices and 2) generalize tropical geometrical objects. We 
    conclude by outlining some future research directions.

_Tropical geometry and deep neural networks_: http://arxiv.org/abs/1805.07091

    Abstract. We establish, for the first time, connections between feedforward
    neural networks with ReLU activation and tropical geometry - we show that
    the family of such neural networks is equivalent to the family of tropical 
    rational maps. Among other things, we deduce that feedforward ReLU neural 
    networks with one hidden layer can be characterized by zonotopes, which 
    serve as building blocks for deeper networks; we relate decision boundaries 
    of such neural networks to tropical hypersurfaces, a major object of study 
    in tropical geometry; and we prove that linear regions of such neural 
    networks correspond to vertices of polytopes associated with tropical 
    rational functions. An insight from our tropical formulation is that a 
    deeper network is exponentially more expressive than a shallow network.

_Topology of deep neural networks_: http://arxiv.org/abs/2004.06093

    The authors study how the topology of a data set representing two classes
    of objects in a binary classification problem changes as it passes through 
    the layers of a well-trained neural network. The goal is to shed light on 
    two well-known mysteries in deep neural networks:
    (i) a nonsmooth activation function like ReLU outperforms a smooth one 
    like hyperbolic tangent, 
    (ii) successful neural network architectures rely on having many layers, 
    despite the fact that a shallow network is able to approximate any function
    arbitrary well.
    
    Their results demonstrate the following: 
    (1) Neural networks operate by changing topology, transforming a topologically 
    complicated data set into a topologically simple one as it passes through 
    the layers.
    (2) the reduction in Betti numbers is significantly faster for ReLU 
    activation compared to hyperbolic tangent activation as the former defines 
    nonhomeomorphic maps that change topology, whereas the latter defines 
    homeomorphic maps that preserve topology. 
    (3) shallow and deep networks transform the same data set somewhat differently
    - a shallow network operates mainly through changing geometry and changes
    topology only in its final layers, a deep one spreads topological changes 
    more evenly across all layer.

Negut, _Category O for quantum loop algebras_: http://arxiv.org/pdf/2501.00724

    Abstract. We generalize the Hernandez-Jimbo category O of representations
    of Borel subalgebras of quantum affine algebras to the case of quantum loop 
    algebras for arbitrary Kac-Moody g (as well as related algebras, such as  
    quantum toroidal gl(1)). Moreover, we give explicit realizations of all  
    simple modules, and devise tools for the computation of q-characters that 
    are new even for g of finite type. Our techniques allow us to generalize  
    classic results of FrenkelHernandez, Frenkel-Mukhin, Hernandez-Jimbo and 
    Hernandez-Leclerc, as well as prove conjectures of Feigin-Jimbo-Miwa-Mukhin 
    and Mukhin-Young.

---

## for students:

 * _Mathematical Data Science_: http://arxiv.org/pdf/2502.08620

 * _Algorithmic mathematics in machine learning_: http://epubs.siam.org/doi/book/10.1137/1.9781611977882
 
 * _Machine learning and artificial intelligence_: http://link.springer.com/book/10.1007/978-3-031-12282-8
 
 * _The nearly perfect prediction theorem_: http://www.ams.org/journals/notices/202503/rnoti-p308.pdf
 
 * _Operator theory of electrical resistance networks_: http://arxiv.org/pdf/0806.3881
