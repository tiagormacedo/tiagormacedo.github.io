#     ARTICLES TO READ


Tao, _Machine assisted proofs_: http://www.ams.org/journals/notices/202501/rnoti-p6.pdf

    On computer-assisted mathematics: the instances in which machine learning, 
    proof assistants and large language models have advanced mathematics, and 
    the possibilities of combining these techniques in the future.

_Machine learning, mathematical theory and scientific applications_: http://www.ams.org/journals/notices/201911/rnoti-p1813.pdf

    The article begins with a summary of a few areas in which machine learning has
    been applied in science: partial differential equations, multi-scale modeling,
    molecular dynamics, gas dynamics, and natural language processing. Then, the
    mathematical theory behind regression problems is shortly presented. Two 
    important points that the article highlights:
    
      * "... in theoretical and computational science and engineering, a 
        fundamental obstacle that we have encountered is our limited ability
        to deal with problems in high dimension."
         
      * "To build the theoretical foundation of machine learning, we need to 
        develop high-dimensional numerical analysis."

Hernandez, _Symmetries of Grothendieck rings in representation theory_: http://arxiv.org/pdf/2501.03024

    The article starts with a broad overview on representation theory, 
    category theory, Grothendieck rings and cluster algebras in order to
    present several instances in which Grothendieck rings admit symmetries.

_Learning the symmetric group: large from small_: http://arxiv.org/pdf/2502.12717
    
    In this article, the authors show that a transformer neural-network trained 
    on predicting permutations in the symmetric group on 10 letters from their 
    words (either formed by general transpositions or by adjacent transpositions)
    can generalize to bigger symmetric groups with nearly 100% accuracy. They
    employ identity augmentation as a key tool to manage variable word lengths,
    and partitioned windows for training on the adjacent transpositions case.

_On the geometry of deep learning_: https://www.ams.org/journals/notices/202504/rnoti-p374.pdf

    In this article, the authors describe the connection between deep neural
    networks and function approximation by affine splines (continuous piecewise
    linear functions in multiple dimensions). In particular, they describe how a
    neural network's learning process tessellates its input space. This affine
    spline connection and this geometrical viewpoint are promising ways of
    viewing, analyzing and improving the inner workings of deep neural networks.

_Mathematical discoveries from program search with large language models_: http://www.nature.com/articles/s41586-023-06924-6.pdf

    This paper presents FunSearch, a method that focuses on searching for 
    programs that describe how to solve a problem, rather than directly 
    providing a solution. FunSearch not only offers an effective and scalable 
    strategy, but the programs it discovers are often more interpretable. To 
    achieve this, the problem should allow for an efficient 'evaluate' function 
    that measures the quality of candidate solutions. FunSearch then combines 
    large language models (LLMs) with evolutionary algorithms. The results 
    presented in this article provide evidence that FunSearch’s outcomes are 
    genuinely original, rather than retrieved from the LLM’s training data, as 
    they surpass state-of-the-art results on certain open problems.

Creedon and Mazorchuk, _Consecutive Patterns, Kostant's Problem and Type A6_: https://arxiv.org/pdf/2503.07809

    In this article the authors analyse several related problems and conjectures
    concerning special linear Lie algebras and symmetric groups: Konstant's 
    Problem, Kahrstrom's Conjecture and Indecomposability Conjecture. By relating
    these problems and conjectures, using consecutive pattern techniques, and 
    performing a number of case-by-case computations, the authors prove the 
    veracity of the Incomposability Conjecture for the Lie algebra of type A6.

Dotsenko and Mozgovoy, _Global Weyl modules for thin Lie algebras are finite-dimensional_: https://arxiv.org/pdf/2411.17550.

    In this article, the authors study Weyl modules for Lie algebras that 
    contain a semisimple Lie algebra as a subalgebra and are integrable under 
    their adjoint action. The authors generalise several previous results, 
    including those by Chari, Fourier and Khandai (2010) and Manning, Neher and 
    Salmasian (2018).

    To be more specific, recall that a module for a semisimple Lie algebra is 
    said to be integrable when it can be decomposed as a direct sum of its 
    finite-dimensional submodules. Let g̅ be a Lie algebra. Assume that g̅ 
    contains a Lie subalgebra of g that is a semisimple Lie algebra. In this 
    article, the authors define global Weyl modules for g̅ as maximal integrable
    g̅-modules which are integrable when restricted to g and which are generated,
    as g̅-modules, by a highest-weight vector. They also prove a finite-
    -dimensionality result for global Weyl modules and construct stratifications 
    in certain categories of integrable g̅-modules.

Rani and Setia, _Graded representations of current Lie superalgebras sl(1,2)[t]_: https://arxiv.org/pdf/2506.01134.

    In this article, Rani and Setia advance our understanding of local Weyl 
    modules and Chari-Venkatesh modules for the current superalgebra of sl(1|2).
    In fact, they introduce a combinatorial technique to parametrize the bases
    of local Weyl modules, they also provide a graded character formula for
    graded local Weyl modules, and most interestingly, they construct short 
    exact sequences of Chari-Venkatesh modules that enable us to filter Chari-
    -Venkatesh modules by other Chari-Venkatesh modules.

Chirvasitu, _Finite-dimensional modules over associative equivariant map algebras_: https://arxiv.org/pdf/2509.01386.

    In this article, the author classifies finite-dimensional representations of 
    equivariant map _associative_ algebras. To do that, he uses several results 
    from category theory and on representations of algebraic groups and Hopf 
    algebras.

---

Vaswani et al., _Attention is all you need_: https://dl.acm.org/doi/pdf/10.5555/3295222.3295349

Olanrewaju, _Partial Symmetry Enforced Attention Decomposition_: https://arxiv.org/pdf/2507.14908

_Tropical geometry and machine learning_: http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9394420

---

## for students:

 * _Mathematical Data Science_: http://arxiv.org/pdf/2502.08620

 * _Machine Learning meets Algebraic Combinatorics_: http://arxiv.org/pdf/2503.06366
  
 * _The nearly perfect prediction theorem_: http://www.ams.org/journals/notices/202503/rnoti-p308.pdf
 
 * _Tropical geometry and deep neural networks_: http://arxiv.org/abs/1805.07091
